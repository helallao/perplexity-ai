"""
fastmcp-based MCP server exposing Perplexity search and model listing tools.
Provides both stdio (console) and HTTP transports.
"""

import argparse
import os
from typing import Any, Dict, Iterable, List, Optional, Union

from fastmcp import FastMCP
from fastmcp.server.middleware import Middleware, MiddlewareContext
from fastmcp.server.dependencies import get_http_headers

from .client import Client
from .config import LABS_MODELS, MODEL_MAPPINGS, SEARCH_LANGUAGES, SEARCH_MODES, SEARCH_SOURCES
from .exceptions import ValidationError
from .utils import sanitize_query, validate_file_data, validate_query_limits, validate_search_params

# API å¯†é’¥é…ç½®ï¼ˆä»ŽçŽ¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤ä¸º sk-123456ï¼‰
MCP_TOKEN = os.getenv("MCP_TOKEN", "sk-123456")


class AuthMiddleware(Middleware):
    """Bearer Token è®¤è¯ä¸­é—´ä»¶"""
    
    def __init__(self, token: str):
        self.token = token
    
    async def on_request(self, context: MiddlewareContext, call_next):
        """éªŒè¯è¯·æ±‚çš„ Authorization header"""
        headers = get_http_headers()
        if headers:  # HTTP æ¨¡å¼ä¸‹æ‰æœ‰ headers
            auth = headers.get("authorization") or headers.get("Authorization")
            if auth != f"Bearer {self.token}":
                raise PermissionError("Unauthorized: Invalid or missing Bearer token")
        return await call_next(context)


mcp = FastMCP("perplexity-mcp")

# æ·»åŠ è®¤è¯ä¸­é—´ä»¶
mcp.add_middleware(AuthMiddleware(MCP_TOKEN))
_client: Optional[Client] = None


def _get_client() -> Client:
    """Create a singleton Client instance."""
    global _client
    if _client is None:
        csrf_token = os.getenv("PPLX_NEXT_AUTH_CSRF_TOKEN")
        session_token = os.getenv("PPLX_SESSION_TOKEN")
        cookies = (
            {
                "next-auth.csrf-token": csrf_token,
                "__Secure-next-auth.session-token": session_token,
            }
            if csrf_token and session_token
            else {}
        )
        _client = Client(cookies)
    return _client


def _normalize_files(files: Optional[Union[Dict[str, Any], Iterable[str]]]) -> Dict[str, Any]:
    """
    Accept either a dict of filename->data or an iterable of file paths,
    and normalize to the dict format expected by Client.search.
    """
    if not files:
        return {}

    if isinstance(files, dict):
        normalized = files
    else:
        normalized = {}
        for path in files:
            filename = os.path.basename(path)
            with open(path, "rb") as fh:
                normalized[filename] = fh.read()

    validate_file_data(normalized)
    return normalized


def list_models_tool() -> Dict[str, Any]:
    """Return supported modes, model mappings, and Labs models."""
    return {
        "modes": SEARCH_MODES,
        "model_mappings": MODEL_MAPPINGS,
        "labs_models": LABS_MODELS,
    }


def _extract_clean_result(response: Dict[str, Any]) -> Dict[str, Any]:
    """Extract only the final answer from the search response."""
    result = {}
    
    # åªæå–æœ€ç»ˆç­”æ¡ˆ
    if "answer" in response:
        result["answer"] = response["answer"]
    
    return result


def _run_query(
    query: str,
    mode: str,
    model: Optional[str] = None,
    sources: Optional[List[str]] = None,
    language: str = "en-US",
    incognito: bool = False,
    files: Optional[Union[Dict[str, Any], Iterable[str]]] = None,
) -> Dict[str, Any]:
    """Execute a Perplexity query (non-streaming) and return the final response."""
    client = _get_client()
    try:
        clean_query = sanitize_query(query)
        chosen_sources = sources or ["web"]

        if language not in SEARCH_LANGUAGES:
            raise ValidationError(
                f"Invalid language '{language}'. Choose from: {', '.join(SEARCH_LANGUAGES)}"
            )

        validate_search_params(mode, model, chosen_sources, own_account=client.own)
        normalized_files = _normalize_files(files)
        validate_query_limits(client.copilot, client.file_upload, mode, len(normalized_files))

        response = client.search(
            clean_query,
            mode=mode,
            model=model,
            sources=chosen_sources,
            files=normalized_files,
            stream=False,
            language=language,
            incognito=incognito,
        )
        
        # åªè¿”å›žç²¾ç®€çš„æœ€ç»ˆç»“æžœ
        clean_result = _extract_clean_result(response)
        return {"status": "ok", "data": clean_result}
    except Exception as exc:  # fastmcp will surface this to the client
        return {
            "status": "error",
            "error_type": exc.__class__.__name__,
            "message": str(exc),
        }


@mcp.tool
def list_models() -> Dict[str, Any]:
    """
    èŽ·å– Perplexity æ”¯æŒçš„æ‰€æœ‰æœç´¢æ¨¡å¼å’Œæ¨¡åž‹åˆ—è¡¨
    
    å½“ä½ éœ€è¦äº†è§£å¯ç”¨çš„æ¨¡åž‹é€‰é¡¹æ—¶è°ƒç”¨æ­¤å·¥å…·ã€‚
    
    Returns:
        åŒ…å« modes (æœç´¢æ¨¡å¼)ã€model_mappings (æ¨¡åž‹æ˜ å°„) å’Œ labs_models (å®žéªŒæ¨¡åž‹) çš„å­—å…¸
    """
    return list_models_tool()


@mcp.tool
def search(
    query: str,
    mode: str = "pro",
    model: Optional[str] = None,
    sources: Optional[List[str]] = None,
    language: str = "en-US",
    incognito: bool = False,
    files: Optional[Union[Dict[str, Any], Iterable[str]]] = None,
) -> Dict[str, Any]:
    """
    Perplexity å¿«é€Ÿæœç´¢ - ç”¨äºŽèŽ·å–å®žæ—¶ç½‘ç»œä¿¡æ¯å’Œç®€å•é—®é¢˜è§£ç­”

    âš¡ ç‰¹ç‚¹: é€Ÿåº¦å¿«ï¼Œé€‚åˆéœ€è¦å®žæ—¶ä¿¡æ¯çš„ç®€å•æŸ¥è¯¢

    Args:
        query: æœç´¢é—®é¢˜ (æ¸…æ™°ã€å…·ä½“çš„é—®é¢˜æ•ˆæžœæ›´å¥½)
        mode: æœç´¢æ¨¡å¼
            - 'auto': å¿«é€Ÿæ¨¡å¼ï¼Œä½¿ç”¨ turbo æ¨¡åž‹ï¼Œä¸æ¶ˆè€—é¢åº¦
            - 'pro': ä¸“ä¸šæ¨¡å¼ï¼Œæ›´å‡†ç¡®çš„ç»“æžœ (é»˜è®¤)
        model: æŒ‡å®šæ¨¡åž‹ (ä»… pro æ¨¡å¼ç”Ÿæ•ˆ)
            - None: ä½¿ç”¨é»˜è®¤æ¨¡åž‹ (æŽ¨è)
            - 'sonar': Perplexity è‡ªç ”æ¨¡åž‹
            - 'gpt-5.2': OpenAI æœ€æ–°æ¨¡åž‹
            - 'claude-4.5-sonnet': Anthropic Claude
            - 'grok-4.1': xAI Grok
        sources: æœç´¢æ¥æºåˆ—è¡¨
            - 'web': ç½‘é¡µæœç´¢ (é»˜è®¤)
            - 'scholar': å­¦æœ¯è®ºæ–‡
            - 'social': ç¤¾äº¤åª’ä½“
        language: å“åº”è¯­è¨€ä»£ç  (é»˜è®¤ 'en-US'ï¼Œä¸­æ–‡ç”¨ 'zh-CN')
        incognito: éšèº«æ¨¡å¼ï¼Œä¸ä¿å­˜æœç´¢åŽ†å²
        files: ä¸Šä¼ æ–‡ä»¶ (ç”¨äºŽåˆ†æžæ–‡æ¡£å†…å®¹)

    Returns:
        {"status": "ok", "data": {"answer": "æœç´¢ç»“æžœ..."}}
        æˆ– {"status": "error", "error_type": "...", "message": "..."}
    """
    # é™åˆ¶ search åªèƒ½ä½¿ç”¨ auto æˆ– pro æ¨¡å¼
    if mode not in ["auto", "pro"]:
        mode = "pro"
    return _run_query(query, mode, model, sources, language, incognito, files)


@mcp.tool
def research(
    query: str,
    mode: str = "reasoning",
    model: Optional[str] = "gemini-3.0-pro",
    sources: Optional[List[str]] = None,
    language: str = "en-US",
    incognito: bool = False,
    files: Optional[Union[Dict[str, Any], Iterable[str]]] = None,
) -> Dict[str, Any]:
    """
    Perplexity æ·±åº¦ç ”ç©¶ - ç”¨äºŽå¤æ‚é—®é¢˜åˆ†æžå’Œæ·±åº¦è°ƒç ”

    ðŸ§  ç‰¹ç‚¹: ä½¿ç”¨æŽ¨ç†æ¨¡åž‹ï¼Œä¼šè¿›è¡Œå¤šæ­¥æ€è€ƒï¼Œç»“æžœæ›´å…¨é¢å‡†ç¡®ï¼Œä½†è€—æ—¶è¾ƒé•¿

    Args:
        query: ç ”ç©¶é—®é¢˜ (é—®é¢˜è¶Šå…·ä½“ï¼Œç ”ç©¶ç»“æžœè¶Šæœ‰é’ˆå¯¹æ€§)
        mode: ç ”ç©¶æ¨¡å¼
            - 'reasoning': æŽ¨ç†æ¨¡å¼ï¼Œå¤šæ­¥æ€è€ƒåˆ†æž (é»˜è®¤)
            - 'deep research': æ·±åº¦ç ”ç©¶ï¼Œæœ€å…¨é¢ä½†æœ€è€—æ—¶
        model: æŒ‡å®šæŽ¨ç†æ¨¡åž‹ (ä»… reasoning æ¨¡å¼ç”Ÿæ•ˆ)
            - 'gemini-3.0-pro': Google Gemini Pro (é»˜è®¤ï¼ŒæŽ¨è)
            - 'gpt-5.2-thinking': OpenAI æ€è€ƒæ¨¡åž‹
            - 'claude-4.5-sonnet-thinking': Claude æŽ¨ç†æ¨¡åž‹
            - 'kimi-k2-thinking': Moonshot Kimi
            - 'grok-4.1-reasoning': xAI Grok æŽ¨ç†
        sources: æœç´¢æ¥æºåˆ—è¡¨
            - 'web': ç½‘é¡µæœç´¢ (é»˜è®¤)
            - 'scholar': å­¦æœ¯è®ºæ–‡ (å­¦æœ¯ç ”ç©¶æŽ¨è)
            - 'social': ç¤¾äº¤åª’ä½“
        language: å“åº”è¯­è¨€ä»£ç  (é»˜è®¤ 'en-US'ï¼Œä¸­æ–‡ç”¨ 'zh-CN')
        incognito: éšèº«æ¨¡å¼ï¼Œä¸ä¿å­˜æœç´¢åŽ†å²
        files: ä¸Šä¼ æ–‡ä»¶ (ç”¨äºŽåˆ†æžæ–‡æ¡£å†…å®¹)

    Returns:
        {"status": "ok", "data": {"answer": "ç ”ç©¶ç»“æžœ..."}}
        æˆ– {"status": "error", "error_type": "...", "message": "..."}
    """
    # é™åˆ¶ research åªèƒ½ä½¿ç”¨ reasoning æˆ– deep research æ¨¡å¼
    if mode not in ["reasoning", "deep research"]:
        mode = "reasoning"
    return _run_query(query, mode, model, sources, language, incognito, files)


def run_server(
    transport: str = "stdio",
    host: str = "127.0.0.1",
    port: int = 8000,
) -> None:
    """Start the MCP server with the requested transport."""
    if transport == "http":
        mcp.run(transport="http", host=host, port=port)
    else:
        mcp.run()


def main() -> None:
    parser = argparse.ArgumentParser(description="Run Perplexity MCP server (fastmcp).")
    parser.add_argument(
        "--transport",
        choices=["stdio", "http"],
        default="stdio",
        help="Transport to use for MCP server.",
    )
    parser.add_argument("--host", default="127.0.0.1", help="HTTP host (when transport=http).")
    parser.add_argument("--port", type=int, default=8000, help="HTTP port (when transport=http).")
    args = parser.parse_args()
    run_server(transport=args.transport, host=args.host, port=args.port)


if __name__ == "__main__":
    main()

